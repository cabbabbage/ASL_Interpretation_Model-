{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Cal\\miniconda3\\envs\\cs456\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import concurrent.futures\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# Configurable variables and constants\n",
    "LOOK_AHEAD = 3\n",
    "SENSITIVITY = 0.3\n",
    "MULTIPLIER = 200\n",
    "SEQUENCE_LENGTH = 30\n",
    "NO_SEQUENCES = 30\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = './actions'\n",
    "CSV_FILE = 'action_directory.csv'\n",
    "VIDEOS_PATH = './videos'\n",
    "\n",
    "# Mediapipe constants\n",
    "NUM_LANDMARKS_HAND = 21 * 3  # Each hand has 21 landmarks with x, y, z coordinates\n",
    "\n",
    "# Mediapipe models and drawing utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset Directories, video files, directory (BE CAREFUL RUNNING THIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    \"\"\"Reset the environment by optionally deleting and recreating the DATA_PATH.\"\"\"\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        # Count the number of '.mp4' files in the 'actions' directory\n",
    "        if os.path.exists(DATA_PATH):\n",
    "            mp4_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.mp4')]\n",
    "            num_mp4_files = len(mp4_files)\n",
    "        else:\n",
    "            num_mp4_files = 0\n",
    "\n",
    "        # Prompt the user if they want to reset the directory\n",
    "        reset_choice = input(f\"Would you like to reset? There are {num_mp4_files} mp4 files in '{DATA_PATH}'. Enter Y to reset or N to exit: \")\n",
    "        if reset_choice.strip().upper() == 'Y':\n",
    "            # Proceed with reset\n",
    "            __import__('subprocess').run(['python', './make_directory.py'])\n",
    "\n",
    "            if os.path.exists(DATA_PATH):\n",
    "                shutil.rmtree(DATA_PATH)\n",
    "                print(f\"Deleted existing directory: {DATA_PATH}\")\n",
    "            os.makedirs(DATA_PATH, exist_ok=True)\n",
    "            print(f\"Created new directory: {DATA_PATH}\")\n",
    "            process_csv_videos()\n",
    "            setup_action_directories()\n",
    "        else:\n",
    "            print(\"Exiting without resetting.\")\n",
    "            process_csv_videos()\n",
    "            return\n",
    "    else:\n",
    "        # If 'action_directory.csv' doesn't exist, proceed with environment setup\n",
    "        os.makedirs(DATA_PATH, exist_ok=True)\n",
    "        __import__('subprocess').run(['python', './make_directory.py'])\n",
    "        print(f\"Created new directory: {DATA_PATH}\")\n",
    "        process_csv_videos()\n",
    "        setup_action_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous Functions and Video Editing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_action_directories():\n",
    "    \"\"\"Set up directories for each action and sequence.\"\"\"\n",
    "    actions = [f.replace('.mp4', '') for f in os.listdir(DATA_PATH) if f.endswith('.mp4')]\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        os.makedirs(action_path, exist_ok=True)\n",
    "        for sequence in range(NO_SEQUENCES):\n",
    "            sequence_path = os.path.join(action_path, f'seq_{sequence}')\n",
    "            os.makedirs(sequence_path, exist_ok=True)\n",
    "\n",
    "def process_csv_videos():\n",
    "    \"\"\"Process videos based on the CSV file and generate trimmed videos.\"\"\"\n",
    "    # Ask the user how many videos they want to process\n",
    "    try:\n",
    "        num_videos_to_process = int(input(\"How many actions would you like to process? Enter 0 for all: \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a valid number.\")\n",
    "        return\n",
    "\n",
    "    # Ask if the user wants to overwrite existing videos\n",
    "    overwrite_response = input(\"Would you like to overwrite existing videos? Enter Y for yes or N for no: \")\n",
    "    video_overwrite = overwrite_response.strip().upper() == 'Y'\n",
    "\n",
    "    with open(CSV_FILE, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        os.makedirs(DATA_PATH)\n",
    "\n",
    "    # Prepare rows to process\n",
    "    if video_overwrite:\n",
    "        rows_to_process = [(idx, row) for idx, row in enumerate(rows)]\n",
    "    else:\n",
    "        rows_to_process = [(idx, row) for idx, row in enumerate(rows) if row[0] != \"1\"]\n",
    "\n",
    "    total_videos = len(rows_to_process)\n",
    "\n",
    "    if num_videos_to_process == 0 or num_videos_to_process > total_videos:\n",
    "        num_videos_to_process = total_videos  # Process all available videos\n",
    "\n",
    "    print(f\"Processing {num_videos_to_process} action(s)...\")\n",
    "\n",
    "    processed_videos = 0\n",
    "\n",
    "    for idx_row in rows_to_process:\n",
    "        if processed_videos >= num_videos_to_process:\n",
    "            break  # Stop when we've processed the requested number of videos\n",
    "\n",
    "        row_idx, row = idx_row\n",
    "        process_video_files(row)\n",
    "        processed_videos += 1\n",
    "        print(f\"Processed action {processed_videos}/{num_videos_to_process}\")\n",
    "\n",
    "    # Mark rows as processed\n",
    "    for idx_row in rows_to_process[:num_videos_to_process]:\n",
    "        row_idx, row = idx_row\n",
    "        rows[row_idx][0] = \"1\"\n",
    "\n",
    "    # Save the updated CSV\n",
    "    with open(CSV_FILE, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "def video_exists(video_path):\n",
    "    \"\"\"Check if a video file exists.\"\"\"\n",
    "    return os.path.exists(video_path)\n",
    "\n",
    "def calculate_hand_movement(prev_landmarks, curr_landmarks):\n",
    "    \"\"\"Calculate the movement between hand landmarks.\"\"\"\n",
    "    prev = np.array(prev_landmarks)\n",
    "    curr = np.array(curr_landmarks)\n",
    "\n",
    "    if prev.shape != curr.shape or prev.size == 0 or curr.size == 0:\n",
    "        return True\n",
    "\n",
    "    movement = np.linalg.norm(prev - curr, axis=1)\n",
    "    return np.mean(movement)\n",
    "\n",
    "def extract_hand_landmarks(results):\n",
    "    \"\"\"Extract hand landmarks from Mediapipe results.\"\"\"\n",
    "    left_hand = [[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark] if results.left_hand_landmarks else []\n",
    "    right_hand = [[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark] if results.right_hand_landmarks else []\n",
    "    return left_hand + right_hand\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"Perform Mediapipe detection on an image.\"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    # Keep image in RGB for drawing\n",
    "    return image, results\n",
    "\n",
    "def trim_video_based_on_hand_movement(video_path):\n",
    "    \"\"\"Trim the video based on hand movement detection.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_num = 0\n",
    "\n",
    "    # Read all frames into a list\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "        frame_num += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    total_frames = len(frames)\n",
    "\n",
    "    # Initialize Mediapipe model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as model:\n",
    "        # Find the start frame by analyzing from the beginning\n",
    "        start_frame = 0\n",
    "        recording = False\n",
    "        prev_hand_landmarks = None\n",
    "\n",
    "        for frame_num, frame in enumerate(frames):\n",
    "            _, results = mediapipe_detection(frame, model)\n",
    "            current_hand_landmarks = extract_hand_landmarks(results)\n",
    "\n",
    "            if current_hand_landmarks:\n",
    "                if prev_hand_landmarks is not None:\n",
    "                    movement = calculate_hand_movement(prev_hand_landmarks, current_hand_landmarks)\n",
    "                    if movement * MULTIPLIER >= SENSITIVITY and not recording:\n",
    "                        start_frame = frame_num\n",
    "                        recording = True\n",
    "                        break  # Start frame found; exit loop\n",
    "                prev_hand_landmarks = current_hand_landmarks\n",
    "\n",
    "        # Find the end frame by analyzing from the end\n",
    "        end_frame = total_frames - 1\n",
    "        recording = False\n",
    "        prev_hand_landmarks = None\n",
    "\n",
    "        for reverse_frame_num, frame in enumerate(reversed(frames)):\n",
    "            frame_num = total_frames - 1 - reverse_frame_num\n",
    "            _, results = mediapipe_detection(frame, model)\n",
    "            current_hand_landmarks = extract_hand_landmarks(results)\n",
    "\n",
    "            if current_hand_landmarks:\n",
    "                if prev_hand_landmarks is not None:\n",
    "                    movement = calculate_hand_movement(prev_hand_landmarks, current_hand_landmarks)\n",
    "                    if movement * MULTIPLIER >= SENSITIVITY and not recording:\n",
    "                        end_frame = frame_num\n",
    "                        recording = True\n",
    "                        break  # End frame found; exit loop\n",
    "                prev_hand_landmarks = current_hand_landmarks\n",
    "\n",
    "    return start_frame, end_frame, total_frames\n",
    "\n",
    "def trim_video_frames(video_path, start_frame, end_frame):\n",
    "    \"\"\"Trim the video between the specified start and end frames.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    trimmed_frames = []\n",
    "    current_frame = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if start_frame <= current_frame <= end_frame:\n",
    "            trimmed_frames.append(frame)\n",
    "        current_frame += 1\n",
    "    cap.release()\n",
    "    return trimmed_frames\n",
    "\n",
    "def concatenate_videos(video_frames_list, output_path, fps=30): \n",
    "    \"\"\"Concatenate multiple videos by resizing frames with aspect ratio preservation.\"\"\"\n",
    "    if not video_frames_list or len(video_frames_list) == 0:\n",
    "        print(f\"No video frames to concatenate for {output_path}\")\n",
    "        return\n",
    "\n",
    "    # Set the target size\n",
    "    target_width = 640\n",
    "    target_height = 480\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (target_width, target_height))\n",
    "\n",
    "    for video_frames in video_frames_list:\n",
    "        for frame in video_frames:\n",
    "            # Get current frame dimensions\n",
    "            h, w = frame.shape[:2]\n",
    "\n",
    "            # Compute scaling factor to maintain aspect ratio\n",
    "            scale = min(target_width / w, target_height / h)\n",
    "            new_w = int(w * scale)\n",
    "            new_h = int(h * scale)\n",
    "\n",
    "            # Resize frame\n",
    "            resized_frame = cv2.resize(frame, (new_w, new_h))\n",
    "\n",
    "            # Create a black canvas of target size\n",
    "            canvas = np.zeros((target_height, target_width, 3), dtype=np.uint8)\n",
    "\n",
    "            # Compute top-left corner to center the image\n",
    "            x_offset = (target_width - new_w) // 2\n",
    "            y_offset = (target_height - new_h) // 2\n",
    "\n",
    "            # Place the resized frame onto the canvas\n",
    "            canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_frame\n",
    "\n",
    "            out.write(canvas)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved successfully at {output_path}\")\n",
    "\n",
    "def process_video_files(row):\n",
    "    \"\"\"Process each video listed in the CSV for the action.\"\"\"\n",
    "    action = row[1]\n",
    "    print(f\"Creating video for '{action}'\")\n",
    "    concatenated_frames = []\n",
    "\n",
    "    for video_file in row[2:]:\n",
    "        video_path = os.path.join(VIDEOS_PATH, video_file)\n",
    "        if video_exists(video_path):\n",
    "            start_frame, end_frame, total_frames = trim_video_based_on_hand_movement(video_path)\n",
    "            if end_frame - start_frame > 14:\n",
    "                trimmed_frames = trim_video_frames(video_path, start_frame, end_frame)\n",
    "                concatenated_frames.append(trimmed_frames)\n",
    "\n",
    "                # Calculate the number of frames trimmed\n",
    "                trimmed_frames_start = start_frame\n",
    "                trimmed_frames_end = total_frames - end_frame - 1\n",
    "\n",
    "                # Print the trimming information\n",
    "                print(f\"For action '{action}', video '{video_file}': Trimmed {trimmed_frames_start} frames from the beginning and {trimmed_frames_end} frames from the end.\")\n",
    "            else:\n",
    "                print(f\"Trimmed video for '{video_file}' is too short. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Video file '{video_file}' does not exist. Skipping.\")\n",
    "\n",
    "    output_video_path = os.path.join(DATA_PATH, f\"{action}.mp4\")\n",
    "    concatenate_videos(concatenated_frames, output_path=output_video_path)\n",
    "    return output_video_path\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    \"\"\"Extract keypoints from Mediapipe results.\"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468 * 3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "        mp_drawing.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),\n",
    "        mp_drawing.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1)\n",
    "    )\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Movement Data from Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_video(action):\n",
    "    \"\"\"Process a single action video and extract keypoints without displaying images.\"\"\"\n",
    "    no_sequences = NO_SEQUENCES\n",
    "    sequence_length = SEQUENCE_LENGTH\n",
    "\n",
    "    video_path = os.path.join(DATA_PATH, f'{action}.mp4')\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames <= 0:\n",
    "        print(f\"Error: Video {video_path} has no frames.\")\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.8, \n",
    "                              min_tracking_confidence=0.8) as holistic:\n",
    "        for sequence in range(no_sequences):\n",
    "            frame_count = 0  # Counter for frames in the current sequence\n",
    "\n",
    "            while frame_count < sequence_length:\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                if not ret:\n",
    "                    # If we reach the end of the video, loop back to the beginning\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        print(f\"Error: Unable to read frame from video {video_path}\")\n",
    "                        break\n",
    "\n",
    "                # Perform Mediapipe detection (image is in RGB)\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Extract keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "\n",
    "                # Save keypoints\n",
    "                npy_path = os.path.join(DATA_PATH, action, f'seq_{sequence}', f'frame_{frame_count}.npy')\n",
    "                os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                frame_count += 1  # Increment the frame counter\n",
    "\n",
    "        cap.release()\n",
    "    print(f\"Finished processing video: {video_path}\")\n",
    "\n",
    "def process_videos():\n",
    "    \"\"\"Process all action videos concurrently using multiprocessing.\"\"\"\n",
    "    actions = [f.replace('.mp4', '') for f in os.listdir(DATA_PATH) if f.endswith('.mp4')]\n",
    "\n",
    "    if not actions:\n",
    "        print(\"No action videos found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing {len(actions)} videos concurrently...\")\n",
    "\n",
    "    # Use multiprocessing with a pool of 5 workers\n",
    "    with Pool(processes=5) as pool:\n",
    "        pool.map(process_single_video, actions)\n",
    "\n",
    "    print(\"Finished processing all videos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_model():\n",
    "    # Define the actions (labels) based on the folders in the DATA_PATH\n",
    "    DATA_PATH = 'actions' \n",
    "    actions = [action for action in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, action))]\n",
    "    label_map = {label: num for num, label in enumerate(actions)}\n",
    "    sequence_length = 30  # Set your sequence length\n",
    "\n",
    "    sequences, labels = [], []\n",
    "    for action in actions:\n",
    "        # List all the sequence folders (e.g., seq_0, seq_1, ..., seq_n) for each action\n",
    "        sequence_dirs = [d for d in os.listdir(os.path.join(DATA_PATH, action)) if d.startswith('seq_')]\n",
    "        \n",
    "        action_sequences = []\n",
    "        # Loop through each sequence folder (seq_0, seq_1, ...)\n",
    "        for sequence_dir in sequence_dirs:\n",
    "            window = []  # This will hold all frames (keypoints) for the current sequence\n",
    "\n",
    "            # Loop through the frames in the sequence\n",
    "            for frame_num in range(sequence_length):\n",
    "                # Load the keypoint data (.npy file) for the current frame\n",
    "                npy_path = os.path.join(DATA_PATH, action, sequence_dir, f\"frame_{frame_num}.npy\")\n",
    "                res = np.load(npy_path)  # Load the numpy array\n",
    "                window.append(res)  # Append the keypoints to the window (sequence)\n",
    "\n",
    "            action_sequences.append(window)\n",
    "\n",
    "        # Convert action_sequences to numpy array for processing\n",
    "        action_sequences = np.array(action_sequences)  # Shape: (num_sequences, sequence_length, keypoint_dim)\n",
    "\n",
    "        # Compute the mean sequence for the action\n",
    "        mean_sequence = np.mean(action_sequences, axis=0)  # Shape: (sequence_length, keypoint_dim)\n",
    "\n",
    "        # Compute the MSE between each sequence and the mean sequence\n",
    "        mse_list = []\n",
    "        for seq in action_sequences:\n",
    "            mse = np.mean((seq - mean_sequence) ** 2)\n",
    "            mse_list.append(mse)\n",
    "\n",
    "        # Convert mse_list to numpy array\n",
    "        mse_list = np.array(mse_list)\n",
    "\n",
    "        # Compute threshold for outlier detection (e.g., sequences with MSE > mean + 2*std)\n",
    "        mse_mean = np.mean(mse_list)\n",
    "        mse_std = np.std(mse_list)\n",
    "        threshold = mse_mean + 3 * mse_std  # Adjust the multiplier as needed\n",
    "\n",
    "        # Filter out sequences with high MSE\n",
    "        filtered_sequences = []\n",
    "        for i, seq in enumerate(action_sequences):\n",
    "            mse = mse_list[i]\n",
    "            if mse <= threshold:\n",
    "                sequences.append(seq)\n",
    "                labels.append(label_map[action])\n",
    "            else:\n",
    "                print(f\"Sequence '{sequence_dirs[i]}' in action '{action}' is an outlier and will be removed. MSE: {mse}\")\n",
    "\n",
    "    X = np.array(sequences)\n",
    "    y = to_categorical(labels).astype(int)    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "    log_dir = os.path.join('Logs')\n",
    "    tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(sequence_length, X.shape[2])))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(len(actions), activation='softmax'))\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])\n",
    "    model.summary()\n",
    "\n",
    "    # 1. New detection variables\n",
    "    sequence = []\n",
    "    sentence = []\n",
    "    predictions = []\n",
    "    threshold = 0.6\n",
    "\n",
    "    model = tf.keras.models.load_model('model.h5')\n",
    "    model.load_weights('model_weights.h5')\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs456",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
