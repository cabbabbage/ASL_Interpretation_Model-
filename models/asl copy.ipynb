{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Cal\\miniconda3\\envs\\cs456\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import concurrent.futures\n",
    "import tkinter as tk\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from multiprocessing import Pool\n",
    "from tkinter import filedialog, messagebox, Tk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "# Configurable variables and constants\n",
    "LOOK_AHEAD = 3\n",
    "SENSITIVITY = 0.3\n",
    "MULTIPLIER = 200\n",
    "SEQUENCE_LENGTH = 30\n",
    "NO_SEQUENCES = 30\n",
    "\n",
    "actions = []\n",
    "DATA_PATH = './actions'\n",
    "CSV_FILE = 'action_directory.csv'\n",
    "VIDEOS_PATH = './videos'\n",
    "Model_Name = \"\"\n",
    "\n",
    "# Mediapipe constants\n",
    "NUM_LANDMARKS_HAND = 21 * 3  # Each hand has 21 landmarks with x, y, z coordinates\n",
    "\n",
    "# Mediapipe models and drawing utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous Functions and Video Editing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LANDMARKS_HAND = 21 * 3  # Each hand has 21 landmarks with x, y, z coordinates\n",
    "POSE_LANDMARKS = 33 * 4  # 33 pose landmarks with x, y, z, visibility\n",
    "FACE_LANDMARKS = 10 * 3  # Only 10 eyebrow landmarks (x, y, z)\n",
    "LEFT_HAND_LANDMARKS_START = POSE_LANDMARKS + FACE_LANDMARKS\n",
    "RIGHT_HAND_LANDMARKS_START = LEFT_HAND_LANDMARKS_START + NUM_LANDMARKS_HAND\n",
    "\n",
    "\n",
    "def extract_hand_landmarks(results):\n",
    "    \"\"\"Extract hand landmarks from Mediapipe results.\"\"\"\n",
    "    left_hand = [[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark] if results.left_hand_landmarks else []\n",
    "    right_hand = [[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark] if results.right_hand_landmarks else []\n",
    "    return left_hand + right_hand\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"Perform Mediapipe detection on an image.\"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    # Keep image in RGB for drawing\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def remove_frames_without_hands(sequences):\n",
    "    filtered_sequences = []\n",
    "    for idx, sequence in enumerate(sequences):\n",
    "        sequence_with_hands = remove_frames_without_hands_in_sequence(sequence)\n",
    "        if sequence_with_hands:\n",
    "            filtered_sequences.append(sequence_with_hands)\n",
    "            print(f\"Sequence {idx} retained {len(sequence_with_hands)} frames with visible hands.\")\n",
    "        else:\n",
    "            print(f\"Sequence {idx} has no frames with visible hands.\")\n",
    "    return filtered_sequences\n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    \"\"\"Extract keypoints from Mediapipe results, excluding all face landmarks except eyebrows.\"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)\n",
    "    \n",
    "    # Extract only eyebrow landmarks (adjust indices based on your specific MediaPipe model)\n",
    "    if results.face_landmarks:\n",
    "        eyebrow_indices = [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]  # Example indices; adjust if needed\n",
    "        face = np.array([[results.face_landmarks.landmark[i].x, \n",
    "                          results.face_landmarks.landmark[i].y, \n",
    "                          results.face_landmarks.landmark[i].z] for i in eyebrow_indices]).flatten()\n",
    "    else:\n",
    "        face = np.zeros(10 * 3)  # Size matches number of eyebrow landmarks (adjust based on exact indices used)\n",
    "    \n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    \n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "        mp_drawing.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),\n",
    "        mp_drawing.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1)\n",
    "    )\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine MP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "hands = False\n",
    "# Constants\n",
    "NUM_LANDMARKS_HAND = 21 * 3  # Each hand has 21 landmarks with x, y, z coordinates\n",
    "\n",
    "def data_refine(action_data_path, again = False):\n",
    "    sequences = load_sequences(action_data_path)\n",
    "    \n",
    "    # Remove frames without hands\n",
    "    print(\"\\nRemoving frames without visible hands...\")\n",
    "    sequences = remove_frames_without_hands(sequences)\n",
    "    sequences =   interpolate_and_filter_sequences(sequences, action_data_path)\n",
    "    # Prepare reference features\n",
    "    if not sequences:\n",
    "        print(\"No sequences with visible hands found. Exiting.\")\n",
    "        return\n",
    "    reference_sequence = sequences[0]  # Choose a sequence that contains the main pattern\n",
    "    reference_features = extract_features(reference_sequence)\n",
    "    threshold = 0.6  # Adjust this threshold as needed\n",
    "    \n",
    "    # Process sequences to create new data\n",
    "    new_sequences = []\n",
    "    \n",
    "\n",
    "    pattern_sequences = create_pattern_sequences(sequences, reference_features, threshold, num_sequences=10)\n",
    "    new_sequences.extend(pattern_sequences)\n",
    "    \n",
    "    # Next 20 sequences: Trim existing sequences to 30 frames keeping main movement\n",
    "    trimmed_sequences = trim_sequences_to_pattern(sequences, reference_features, threshold, num_sequences=20)\n",
    "    new_sequences.extend(trimmed_sequences)\n",
    "    \n",
    "    # Ensure we have exactly 30 sequences\n",
    "    new_sequences = new_sequences[:30]\n",
    "    \n",
    "    # Save new sequences\n",
    "    save_sequences(new_sequences, action_data_path)\n",
    "    \n",
    "    # Interpolation and Filtering\n",
    "    print(\"\\nStarting interpolation and filtering...\")\n",
    "\n",
    "    # Ensure all sequences have 30 frames\n",
    "    print(\"\\nEnsuring all sequences have 30 frames...\")\n",
    "    sequences = load_sequences(action_data_path)  \n",
    "    sequences = pad_sequences_to_length(sequences, length=30)\n",
    "    # Save the padded sequences\n",
    "    save_sequences(sequences, action_data_path)\n",
    "    print(\"All sequences have been padded to 30 frames if necessary.\")\n",
    "    if again:\n",
    "        data_refine(action_data_path, True)\n",
    "\n",
    "\n",
    "def load_sequences(action_data_path):\n",
    "    sequences = []\n",
    "    seq_dirs = [d for d in os.listdir(action_data_path) if os.path.isdir(os.path.join(action_data_path, d))]\n",
    "    seq_dirs = sorted(seq_dirs, key=lambda x: int(x.split('_')[1]))  # Ensure consistent order\n",
    "    for seq_dir in seq_dirs:\n",
    "        seq_path = os.path.join(action_data_path, seq_dir)\n",
    "        frames = []\n",
    "        frame_files = sorted([f for f in os.listdir(seq_path) if f.endswith('.npy')],\n",
    "                             key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "        for frame_file in frame_files:\n",
    "            frame_data = np.load(os.path.join(seq_path, frame_file))\n",
    "            frames.append(frame_data)\n",
    "        sequences.append(frames)\n",
    "    return sequences\n",
    "\n",
    "def save_sequences(sequences, action_data_path):\n",
    "    # Remove existing data\n",
    "    shutil.rmtree(action_data_path)\n",
    "    os.makedirs(action_data_path, exist_ok=True)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_dir = os.path.join(action_data_path, f'seq_{i}')\n",
    "        os.makedirs(seq_dir, exist_ok=True)\n",
    "        for j, frame in enumerate(seq):\n",
    "            frame_file = os.path.join(seq_dir, f'frame_{j}.npy')\n",
    "            if frame is not None:\n",
    "                np.save(frame_file, frame)\n",
    "    print(f\"Sequences saved to {action_data_path}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_pattern_sequences(sequences, reference_features, threshold, num_sequences):\n",
    "    new_sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "        seq = select_sequence_with_pattern(sequences, reference_features, threshold)\n",
    "        if seq is None:\n",
    "            break\n",
    "        # Apply constrained variation to maintain realistic movement\n",
    "        seq_variation = add_constrained_variation(seq)\n",
    "        # Trim or extend to ensure each sequence has 30 frames\n",
    "        seq_trimmed = trim_or_extend_sequence(seq_variation, length=30)\n",
    "        new_sequences.append(seq_trimmed)\n",
    "    return new_sequences\n",
    "\n",
    "def compute_similarity(features1, features2):\n",
    "    # Ensure both features are arrays for reshaping\n",
    "    features1 = np.array(features1)\n",
    "    features2 = np.array(features2)\n",
    "    \n",
    "    min_length = min(len(features1), len(features2))\n",
    "    features1 = features1[:min_length].reshape(1, -1)\n",
    "    features2 = features2[:min_length].reshape(1, -1)\n",
    "    similarity = cosine_similarity(features1, features2)[0][0]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_constrained_variation(sequence):\n",
    "    varied_sequence = []\n",
    "    for frame in sequence:\n",
    "        # Smaller variation for hand landmarks and face\n",
    "        hand_variation = np.random.normal(0, 0.003, (NUM_LANDMARKS_HAND,))\n",
    "        face_variation_size = FACE_LANDMARKS  # Match the number of face landmarks\n",
    "        face_variation = np.random.normal(0, 0.001, face_variation_size)\n",
    "        \n",
    "        lh_start = POSE_LANDMARKS + FACE_LANDMARKS  # Left hand starting index\n",
    "        rh_start = lh_start + NUM_LANDMARKS_HAND  # Right hand starting index\n",
    "        frame_variation = frame.copy()\n",
    "\n",
    "        # Check and apply variation for hands if visible\n",
    "        left_hand = frame[lh_start:lh_start + NUM_LANDMARKS_HAND]\n",
    "        right_hand = frame[rh_start:rh_start + NUM_LANDMARKS_HAND]\n",
    "        if not np.all(left_hand == 0):\n",
    "            frame_variation[lh_start:lh_start + NUM_LANDMARKS_HAND] += np.clip(hand_variation, -0.01, 0.01)\n",
    "        if not np.all(right_hand == 0):\n",
    "            frame_variation[rh_start:rh_start + NUM_LANDMARKS_HAND] += np.clip(hand_variation, -0.01, 0.01)\n",
    "        \n",
    "        # Apply face variation only to face landmark region\n",
    "        frame_variation[POSE_LANDMARKS:lh_start] += np.clip(face_variation, -0.005, 0.005)\n",
    "        \n",
    "        varied_sequence.append(frame_variation)\n",
    "    \n",
    "    # Smooth the sequence to ensure realistic transitions\n",
    "    smoothed_sequence = smooth_sequence(varied_sequence)\n",
    "    return smoothed_sequence\n",
    "\n",
    "\n",
    "def smooth_sequence(sequence, alpha=0.7):\n",
    "    # Apply exponential smoothing to reduce sudden, unrealistic movements\n",
    "    smoothed_sequence = [sequence[0]]\n",
    "    for i in range(1, len(sequence)):\n",
    "        smoothed_frame = alpha * sequence[i] + (1 - alpha) * smoothed_sequence[-1]\n",
    "        smoothed_sequence.append(smoothed_frame)\n",
    "    return smoothed_sequence\n",
    "\n",
    "def trim_sequences_to_pattern(sequences, reference_features, threshold, num_sequences):\n",
    "    new_sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "        seq = select_sequence_with_pattern(sequences, reference_features, threshold)\n",
    "        if seq is None:\n",
    "            break\n",
    "        # NEW: Trim sequence to where hands are visible\n",
    "        seq_trimmed = trim_sequence_to_hands_visible(seq, length=30)\n",
    "        new_sequences.append(seq_trimmed)\n",
    "    return new_sequences\n",
    "\n",
    "def reduce_sequences(sequences, reference_features, threshold, num_sequences):\n",
    "    new_sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "        seq = select_sequence_with_pattern(sequences, reference_features, threshold)\n",
    "        if seq is None:\n",
    "            break\n",
    "        # NEW: Remove frames without hands before reducing\n",
    "        seq = remove_frames_without_hands_in_sequence(seq)\n",
    "        seq_reduced = remove_frames_to_length(seq, length=30)\n",
    "        new_sequences.append(seq_reduced)\n",
    "    return new_sequences\n",
    "\n",
    "def select_sequence_with_pattern(sequences, reference_features, threshold):\n",
    "    for seq in sequences:\n",
    "        if contains_main_pattern(seq, reference_features, threshold):\n",
    "            return seq\n",
    "    return None\n",
    "\n",
    "def contains_main_pattern(sequence, reference_features, threshold):\n",
    "    if not sequence:\n",
    "        return False\n",
    "    sequence_features = extract_features(sequence)\n",
    "    similarity = compute_similarity(sequence_features, reference_features)\n",
    "    return similarity >= threshold\n",
    "\n",
    "def extract_features(sequence):\n",
    "    normalized_sequence = normalize_sequence(sequence)\n",
    "    features = np.array([frame.flatten() for frame in normalized_sequence]).flatten()\n",
    "    return features\n",
    "\n",
    "def normalize_sequence(sequence):\n",
    "    return [normalize_frame(frame) for frame in sequence]\n",
    "\n",
    "def normalize_frame(frame):\n",
    "    # Determine the number of keypoints\n",
    "    num_keypoints = frame.shape[0] // 3\n",
    "    # Reshape the frame into (num_keypoints, 3)\n",
    "    frame_reshaped = frame.reshape((num_keypoints, 3))\n",
    "    # Use the first keypoint as the reference point\n",
    "    reference_point = frame_reshaped[0]  # Adjust index as per your data\n",
    "    # Translate keypoints so that reference point is at the origin\n",
    "    translated_frame = frame_reshaped - reference_point\n",
    "    # Compute distances from the origin for scaling\n",
    "    distances = np.linalg.norm(translated_frame, axis=1)\n",
    "    max_distance = np.max(distances)\n",
    "    # Scale the frame if max_distance is greater than zero\n",
    "    if max_distance > 0:\n",
    "        scaled_frame = translated_frame / max_distance\n",
    "    else:\n",
    "        scaled_frame = translated_frame\n",
    "    # Flatten the scaled frame back to 1D\n",
    "    return scaled_frame.flatten()\n",
    "\n",
    "\n",
    "def add_variation(sequence):\n",
    "    return [frame + np.random.normal(0, 0.007, frame.shape) for frame in sequence]\n",
    "\n",
    "def trim_or_extend_sequence(sequence, length):\n",
    "    if len(sequence) > length:\n",
    "        # NEW: Trim frames where hands are not visible\n",
    "        sequence = remove_frames_without_hands_in_sequence(sequence)\n",
    "        if len(sequence) > length:\n",
    "            start = (len(sequence) - length) // 2\n",
    "            return sequence[start:start+length]\n",
    "    # Extend sequence if needed\n",
    "    sequence_extended = sequence.copy()\n",
    "    while len(sequence_extended) < length:\n",
    "        sequence_extended.append(sequence_extended[-1])\n",
    "    return sequence_extended[:length]\n",
    "\n",
    "def trim_sequence_to_main_movement(sequence, reference_features, length):\n",
    "    max_similarity = -1\n",
    "    best_start = 0\n",
    "    sequence = remove_frames_without_hands_in_sequence(sequence)\n",
    "    for start in range(0, len(sequence) - length + 1):\n",
    "        window_sequence = sequence[start:start+length]\n",
    "        window_features = extract_features(window_sequence)\n",
    "        similarity = compute_similarity(window_features, reference_features)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_start = start\n",
    "    return sequence[best_start:best_start+length]\n",
    "\n",
    "# NEW FUNCTION: Trim sequence to where hands are visible and adjust length\n",
    "def trim_sequence_to_hands_visible(sequence, length):\n",
    "    # Remove frames without hands\n",
    "    sequence = remove_frames_without_hands_in_sequence(sequence)\n",
    "    # If sequence is longer than desired length, trim equally from both ends\n",
    "    if len(sequence) > length:\n",
    "        extra_frames = len(sequence) - length\n",
    "        start_trim = extra_frames // 2\n",
    "        end_trim = extra_frames - start_trim\n",
    "        sequence = sequence[start_trim: len(sequence) - end_trim]\n",
    "    # If sequence is shorter, pad it\n",
    "    elif len(sequence) < length:\n",
    "        sequence = pad_sequence(sequence, length)\n",
    "    return sequence\n",
    "\n",
    "# Helper function to pad a sequence\n",
    "def pad_sequence(sequence, length):\n",
    "    sequence_extended = sequence.copy()\n",
    "    while len(sequence_extended) < length:\n",
    "        sequence_extended.append(sequence_extended[-1])\n",
    "    return sequence_extended[:length]\n",
    "\n",
    "def remove_frames_to_length(sequence, length):\n",
    "    if len(sequence) <= length:\n",
    "        return sequence\n",
    "    indices = np.linspace(0, len(sequence) - 1, num=length, dtype=int)\n",
    "    return [sequence[i] for i in indices]\n",
    "\n",
    "def interpolate_and_filter_sequences(sequences, action_data_path):\n",
    "    # Interpolate missing keypoints in each sequence\n",
    "    for idx, sequence in enumerate(sequences):\n",
    "        print(f\"Interpolating missing data in sequence {idx}...\")\n",
    "        sequences[idx] = interpolate_sequence(sequence)\n",
    "    \n",
    "    # Find the representative sequence\n",
    "    try:\n",
    "        print(\"\\nFinding representative sequence...\")\n",
    "        flattened_sequences = [np.array(seq).flatten() for seq in sequences if seq]\n",
    "        if not flattened_sequences:\n",
    "            print(\"No sequences available for finding representative sequence.\")\n",
    "            return\n",
    "        X = np.array(flattened_sequences)\n",
    "        kmedoids = KMedoids(n_clusters=1, metric='euclidean', random_state=0).fit(X)\n",
    "        medoid_index = kmedoids.medoid_indices_[0]\n",
    "        representative_sequence = sequences[medoid_index]\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding representative sequence: {e}\")\n",
    "        representative_sequence = sequences[0]\n",
    "    \n",
    "    # Filter each sequence\n",
    "    for idx, seq in enumerate(sequences):\n",
    "        print(f\"\\nFiltering sequence {idx}...\")\n",
    "        try:\n",
    "            if seq:\n",
    "                filtered_seq = filter_sequence(seq, representative_sequence)\n",
    "                sequences[idx] = filtered_seq\n",
    "            else:\n",
    "                print(f\"Sequence {idx} is empty after interpolation. Skipping filtering.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error filtering sequence {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save the interpolated and filtered sequences\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def interpolate_sequence(sequence):\n",
    "    \"\"\"\n",
    "    Interpolates missing keypoints for hand data in sequences with consecutive zero frames.\n",
    "    \n",
    "    For each frame with zero hand data, this function looks forward and backward until it finds\n",
    "    valid data, then interpolates across all frames in between.\n",
    "    \"\"\"\n",
    "    lh_start = LEFT_HAND_LANDMARKS_START\n",
    "    lh_end = lh_start + NUM_LANDMARKS_HAND\n",
    "    rh_start = RIGHT_HAND_LANDMARKS_START\n",
    "    rh_end = rh_start + NUM_LANDMARKS_HAND\n",
    "\n",
    "    def find_valid_indices(sequence, current_index, idx_start, idx_end):\n",
    "        \"\"\"\n",
    "        Finds the closest previous and next indices with valid data for interpolation.\n",
    "        \"\"\"\n",
    "        prev_idx = next_idx = None\n",
    "\n",
    "        # Look backward for valid previous index\n",
    "        for j in range(current_index - 1, -1, -1):\n",
    "            if not np.all(sequence[j][idx_start:idx_end] == 0):\n",
    "                prev_idx = j\n",
    "                break\n",
    "\n",
    "        # Look forward for valid next index\n",
    "        for j in range(current_index + 1, len(sequence)):\n",
    "            if not np.all(sequence[j][idx_start:idx_end] == 0):\n",
    "                next_idx = j\n",
    "                break\n",
    "\n",
    "        return prev_idx, next_idx\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        frame = sequence[i]\n",
    "\n",
    "        # Interpolate left hand if needed\n",
    "        if np.all(frame[lh_start:lh_end] == 0):\n",
    "            prev_idx, next_idx = find_valid_indices(sequence, i, lh_start, lh_end)\n",
    "            if prev_idx is not None and next_idx is not None:\n",
    "                interpolate_frames(sequence, prev_idx, next_idx, lh_start, lh_end)\n",
    "\n",
    "        # Interpolate right hand if needed\n",
    "        if np.all(frame[rh_start:rh_end] == 0):\n",
    "            prev_idx, next_idx = find_valid_indices(sequence, i, rh_start, rh_end)\n",
    "            if prev_idx is not None and next_idx is not None:\n",
    "                interpolate_frames(sequence, prev_idx, next_idx, rh_start, rh_end)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def interpolate_frames(sequence, prev_idx, next_idx, idx_start, idx_end):\n",
    "    \"\"\"\n",
    "    Interpolates hand data between two indices (prev_idx and next_idx) across the range from\n",
    "    idx_start to idx_end in each frame. Fills in each frame in the gap with linearly interpolated values.\n",
    "    \"\"\"\n",
    "    for interp_idx in range(prev_idx + 1, next_idx):\n",
    "        alpha = (interp_idx - prev_idx) / (next_idx - prev_idx)\n",
    "        sequence[interp_idx][idx_start:idx_end] = (\n",
    "            (1 - alpha) * sequence[prev_idx][idx_start:idx_end] +\n",
    "            alpha * sequence[next_idx][idx_start:idx_end]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_sequence(seq, representative_sequence):\n",
    "    try:\n",
    "        distance, path = fastdtw(seq, representative_sequence, dist=euclidean)\n",
    "        indices_seq = [index[0] for index in path]\n",
    "        segments = []\n",
    "        start = indices_seq[0]\n",
    "        prev = indices_seq[0]\n",
    "        for idx_seq in indices_seq[1:]:\n",
    "            if idx_seq == prev + 1:\n",
    "                prev = idx_seq\n",
    "            else:\n",
    "                segments.append((start, prev))\n",
    "                start = idx_seq\n",
    "                prev = idx_seq\n",
    "        segments.append((start, prev))\n",
    "        longest_segment = max(segments, key=lambda x: x[1] - x[0])\n",
    "        start_idx, end_idx = longest_segment\n",
    "        filtered_seq = [None if i < start_idx or i > end_idx else seq[i] for i in range(len(seq))]\n",
    "        print(f\"Filtered sequence length: {len([f for f in filtered_seq if f is not None])} frames (non-matching frames removed)\")\n",
    "        return filtered_seq\n",
    "    except Exception as e:\n",
    "        print(f\"Error during filtering: {e}\")\n",
    "        return seq\n",
    "\n",
    "# NEW FUNCTION: Pad sequences to a specified length\n",
    "def pad_sequences_to_length(sequences, length=30):\n",
    "    for idx, seq in enumerate(sequences):\n",
    "        # Remove None frames that may have been introduced\n",
    "        seq = [frame for frame in seq if frame is not None]\n",
    "        sequences[idx] = seq\n",
    "        actual_length = len(seq)\n",
    "        if actual_length < length:\n",
    "            frames_to_add = length - actual_length\n",
    "            if actual_length > 0:\n",
    "                last_frame = seq[-1]\n",
    "                seq.extend([last_frame.copy() for _ in range(frames_to_add)])\n",
    "                print(f\"Sequence {idx} padded with {frames_to_add} frame(s).\")\n",
    "            else:\n",
    "                print(f\"Sequence {idx} is empty. Cannot pad an empty sequence.\")\n",
    "        elif actual_length > length:\n",
    "            # Trim the sequence to the desired length\n",
    "            sequences[idx] = seq[:length]\n",
    "    return sequences\n",
    "\n",
    "# NEW FUNCTION: Remove frames without visible hands from all sequences\n",
    "def remove_frames_without_hands(sequences):\n",
    "    for idx, seq in enumerate(sequences):\n",
    "        sequences[idx] = remove_frames_without_hands_in_sequence(seq)\n",
    "    # Remove empty sequences\n",
    "    sequences = [seq for seq in sequences if seq]\n",
    "    return sequences\n",
    "\n",
    "# NEW FUNCTION: Remove frames without visible hands in a single sequence\n",
    "def remove_frames_without_hands_in_sequence(sequence):\n",
    "    lh_start = 33 * 4 + 468 * 3\n",
    "    lh_end = lh_start + NUM_LANDMARKS_HAND\n",
    "    rh_start = lh_end\n",
    "    rh_end = rh_start + NUM_LANDMARKS_HAND\n",
    "    new_sequence = []\n",
    "\n",
    "    for frame in sequence:\n",
    "        left_hand = frame[lh_start:lh_end]\n",
    "        right_hand = frame[rh_start:rh_end]\n",
    "        if not (np.all(left_hand == 0) and np.all(right_hand == 0)):\n",
    "            hands = True\n",
    "            new_sequence.append(frame)\n",
    "    return new_sequence\n",
    "\n",
    "# Constants for Hand Data Extraction\n",
    "NUM_LANDMARKS_HAND = 21 * 3  # Each hand has 21 landmarks with x, y, z coordinates\n",
    "POSE_LANDMARKS = 33 * 4  # 33 pose landmarks with x, y, z, visibility\n",
    "FACE_LANDMARKS = 10 * 3  # Only 10 eyebrow landmarks (x, y, z)\n",
    "LEFT_HAND_LANDMARKS_START = POSE_LANDMARKS + FACE_LANDMARKS\n",
    "RIGHT_HAND_LANDMARKS_START = LEFT_HAND_LANDMARKS_START + NUM_LANDMARKS_HAND\n",
    "\n",
    "def remove_frames_without_hands(sequences):\n",
    "    \"\"\"Filter out frames that do not contain visible hand landmarks.\"\"\"\n",
    "    filtered_sequences = []\n",
    "    for idx, sequence in enumerate(sequences):\n",
    "        sequence_with_hands = remove_frames_without_hands_in_sequence(sequence)\n",
    "        if sequence_with_hands:\n",
    "            filtered_sequences.append(sequence_with_hands)\n",
    "            print(f\"Sequence {idx} retained {len(sequence_with_hands)} frames with visible hands.\")\n",
    "        else:\n",
    "            print(f\"Sequence {idx} has no frames with visible hands.\")\n",
    "    return filtered_sequences\n",
    "\n",
    "def remove_frames_without_hands_in_sequence(sequence):\n",
    "    \"\"\"Remove frames from a sequence where both hands are not visible based on their keypoint values.\"\"\"\n",
    "    lh_start = LEFT_HAND_LANDMARKS_START\n",
    "    lh_end = lh_start + NUM_LANDMARKS_HAND\n",
    "    rh_start = RIGHT_HAND_LANDMARKS_START\n",
    "    rh_end = rh_start + NUM_LANDMARKS_HAND\n",
    "    \n",
    "    new_sequence = []\n",
    "    for frame in sequence:\n",
    "        # Check if left or right hand is visible\n",
    "        left_hand_visible = not np.all(frame[lh_start:lh_end] == 0)\n",
    "        right_hand_visible = not np.all(frame[rh_start:rh_end] == 0)\n",
    "        \n",
    "        if left_hand_visible or right_hand_visible:\n",
    "            new_sequence.append(frame)\n",
    "    \n",
    "    return new_sequence\n",
    "\n",
    "def FINAL_FIX(sequences, threshold=0.005):\n",
    "    \"\"\"\n",
    "    Corrects hand positions if they are missing or are extreme outliers by copying from previous or next frame.\n",
    "\n",
    "    Parameters:\n",
    "    - sequences: list of sequences, where each sequence contains frames with hand and body landmarks.\n",
    "    - threshold: float, distance threshold to identify outlier positions based on standard deviation.\n",
    "\n",
    "    Returns:\n",
    "    - Corrected sequences with hand positions copied from adjacent frames where necessary.\n",
    "    \"\"\"\n",
    "    lh_start = LEFT_HAND_LANDMARKS_START\n",
    "    lh_end = lh_start + NUM_LANDMARKS_HAND\n",
    "    rh_start = RIGHT_HAND_LANDMARKS_START\n",
    "    rh_end = rh_start + NUM_LANDMARKS_HAND\n",
    "\n",
    "    def is_outlier(prev, curr, next_):\n",
    "        \"\"\"Check if the current hand position is an outlier compared to neighboring frames.\"\"\"\n",
    "        distance_prev = np.linalg.norm(curr - prev)\n",
    "        distance_next = np.linalg.norm(curr - next_)\n",
    "        print(f\"Outlier Check - Distances: prev={distance_prev}, next={distance_next}, threshold={threshold}\")\n",
    "        return (distance_prev > threshold) or (distance_next > threshold)\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for i, frame in enumerate(sequence):\n",
    "            # Left hand correction\n",
    "            left_hand = frame[lh_start:lh_end]\n",
    "            if i > 0 and i < len(sequence) - 1:\n",
    "                prev_left_hand = sequence[i - 1][lh_start:lh_end]\n",
    "                next_left_hand = sequence[i + 1][lh_start:lh_end]\n",
    "                if np.all(left_hand == 0) or is_outlier(prev_left_hand, left_hand, next_left_hand):\n",
    "                    # Replace with either the previous or next frame's hand data\n",
    "                    frame[lh_start:lh_end] = prev_left_hand if np.linalg.norm(left_hand - prev_left_hand) < np.linalg.norm(left_hand - next_left_hand) else next_left_hand\n",
    "                    print(f\"Frame {i} (Left Hand) corrected using adjacent frame data.\")\n",
    "\n",
    "            elif i == 0:  # First frame: copy from the next frame if current is zeroed or an outlier\n",
    "                next_left_hand = sequence[i + 1][lh_start:lh_end]\n",
    "                if np.all(left_hand == 0):\n",
    "                    frame[lh_start:lh_end] = next_left_hand\n",
    "                    print(f\"Frame {i} (Left Hand) copied from next frame: {next_left_hand}\")\n",
    "\n",
    "            elif i == len(sequence) - 1:  # Last frame: copy from the previous frame if zeroed or an outlier\n",
    "                prev_left_hand = sequence[i - 1][lh_start:lh_end]\n",
    "                if np.all(left_hand == 0):\n",
    "                    frame[lh_start:lh_end] = prev_left_hand\n",
    "                    print(f\"Frame {i} (Left Hand) copied from previous frame: {prev_left_hand}\")\n",
    "\n",
    "            # Right hand correction\n",
    "            right_hand = frame[rh_start:rh_end]\n",
    "            if i > 0 and i < len(sequence) - 1:\n",
    "                prev_right_hand = sequence[i - 1][rh_start:rh_end]\n",
    "                next_right_hand = sequence[i + 1][rh_start:rh_end]\n",
    "                if np.all(right_hand == 0) or is_outlier(prev_right_hand, right_hand, next_right_hand):\n",
    "                    # Replace with either the previous or next frame's hand data\n",
    "                    frame[rh_start:rh_end] = prev_right_hand if np.linalg.norm(right_hand - prev_right_hand) < np.linalg.norm(right_hand - next_right_hand) else next_right_hand\n",
    "                    print(f\"Frame {i} (Right Hand) corrected using adjacent frame data.\")\n",
    "\n",
    "            elif i == 0:  # First frame: copy from the next frame if current is zeroed or an outlier\n",
    "                next_right_hand = sequence[i + 1][rh_start:rh_end]\n",
    "                if np.all(right_hand == 0):\n",
    "                    frame[rh_start:rh_end] = next_right_hand\n",
    "                    print(f\"Frame {i} (Right Hand) copied from next frame: {next_right_hand}\")\n",
    "\n",
    "            elif i == len(sequence) - 1:  # Last frame: copy from the previous frame if zeroed or an outlier\n",
    "                prev_right_hand = sequence[i - 1][rh_start:rh_end]\n",
    "                if np.all(right_hand == 0):\n",
    "                    frame[rh_start:rh_end] = prev_right_hand\n",
    "                    print(f\"Frame {i} (Right Hand) copied from previous frame: {prev_right_hand}\")\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Movement Data from Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_MP_Extraction(video_path, output_directory):\n",
    "    print(video_path)\n",
    "    print(output_directory)\n",
    "    video_path = \"./videos/\" + video_path\n",
    "    \"\"\"\n",
    "    Extract keypoints from each frame of a video and save as .npy files.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        output_directory (str): Directory to save the output .npy files.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # Exit if video ends\n",
    "\n",
    "            # Perform Mediapipe detection on the frame\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            # Extract keypoints from the frame\n",
    "            keypoints = extract_keypoints(results)\n",
    "\n",
    "            # Define the output filename based on frame number\n",
    "            output_filename = f\"frame_{frame_count}.npy\"\n",
    "            output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "            # Save keypoints as a .npy file\n",
    "            np.save(output_path, keypoints)\n",
    "\n",
    "            frame_count += 1\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (300, 30, 288), y shape: (300, 10)\n",
      "WARNING:tensorflow:From c:\\Users\\Cal\\miniconda3\\envs\\cs456\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Cal\\miniconda3\\envs\\cs456\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From c:\\Users\\Cal\\miniconda3\\envs\\cs456\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Cal\\miniconda3\\envs\\cs456\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "9/9 [==============================] - 5s 104ms/step - loss: 2.2610 - categorical_accuracy: 0.1474 - val_loss: 2.2911 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2362 - categorical_accuracy: 0.1895 - val_loss: 2.2645 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 1.7766 - categorical_accuracy: 0.3719 - val_loss: 2.1454 - val_categorical_accuracy: 0.1333 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 1.4754 - categorical_accuracy: 0.5474 - val_loss: 2.2638 - val_categorical_accuracy: 0.1333 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.9756 - categorical_accuracy: 0.6561 - val_loss: 2.2838 - val_categorical_accuracy: 0.0667 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.7327 - categorical_accuracy: 0.7825 - val_loss: 2.1746 - val_categorical_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.4485 - categorical_accuracy: 0.8421 - val_loss: 2.0139 - val_categorical_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.3566 - categorical_accuracy: 0.8982 - val_loss: 1.9726 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1996 - categorical_accuracy: 0.9404 - val_loss: 1.9070 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1460 - categorical_accuracy: 0.9649 - val_loss: 1.6175 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1843 - categorical_accuracy: 0.9439 - val_loss: 1.3644 - val_categorical_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2206 - categorical_accuracy: 0.9684 - val_loss: 2.2726 - val_categorical_accuracy: 0.0667 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1527 - categorical_accuracy: 0.9719 - val_loss: 2.2607 - val_categorical_accuracy: 0.0667 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1169 - categorical_accuracy: 0.9579 - val_loss: 2.2797 - val_categorical_accuracy: 0.0667 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0497 - categorical_accuracy: 0.9895 - val_loss: 2.2428 - val_categorical_accuracy: 0.0667 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0612 - categorical_accuracy: 0.9860 - val_loss: 2.1304 - val_categorical_accuracy: 0.0667 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1022 - categorical_accuracy: 0.9895 - val_loss: 2.2006 - val_categorical_accuracy: 0.0667 - lr: 2.0000e-04\n",
      "Epoch 18/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1266 - categorical_accuracy: 0.9825 - val_loss: 2.1968 - val_categorical_accuracy: 0.0667 - lr: 2.0000e-04\n",
      "Epoch 19/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0964 - categorical_accuracy: 0.9789 - val_loss: 2.1827 - val_categorical_accuracy: 0.1333 - lr: 2.0000e-04\n",
      "Epoch 20/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0746 - categorical_accuracy: 0.9825 - val_loss: 2.1935 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n",
      "Epoch 21/1000\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0140 - categorical_accuracy: 0.9895 - val_loss: 2.2329 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n",
      "Epoch 22/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.3465 - categorical_accuracy: 0.9684 - val_loss: 2.1339 - val_categorical_accuracy: 0.2667 - lr: 1.0000e-04\n",
      "Epoch 23/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0128 - categorical_accuracy: 0.9965 - val_loss: 2.0546 - val_categorical_accuracy: 0.2667 - lr: 1.0000e-04\n",
      "Epoch 24/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0562 - categorical_accuracy: 0.9825 - val_loss: 2.0054 - val_categorical_accuracy: 0.2667 - lr: 1.0000e-04\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 64)            90368     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 30, 64)            256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 30, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 258346 (1009.16 KB)\n",
      "Trainable params: 257962 (1007.66 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Model saved at: Model_data/1\\1.keras\n",
      "Model weights saved at: Model_data/1\\1_weights.keras\n",
      "X shape: (300, 30, 288), y shape: (300, 10)\n",
      "Epoch 1/1000\n",
      "9/9 [==============================] - 5s 104ms/step - loss: 2.2445 - categorical_accuracy: 0.1754 - val_loss: 2.2348 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 1.7259 - categorical_accuracy: 0.3789 - val_loss: 2.2428 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 1.4108 - categorical_accuracy: 0.4421 - val_loss: 2.2198 - val_categorical_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 1.2602 - categorical_accuracy: 0.5684 - val_loss: 2.0913 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 1.1344 - categorical_accuracy: 0.5930 - val_loss: 2.2819 - val_categorical_accuracy: 0.1333 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.7645 - categorical_accuracy: 0.7368 - val_loss: 2.2149 - val_categorical_accuracy: 0.2667 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.5454 - categorical_accuracy: 0.8386 - val_loss: 2.1052 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.5642 - categorical_accuracy: 0.8561 - val_loss: 2.2981 - val_categorical_accuracy: 0.2667 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.3406 - categorical_accuracy: 0.9053 - val_loss: 1.9619 - val_categorical_accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1957 - categorical_accuracy: 0.9333 - val_loss: 2.0911 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.3348 - categorical_accuracy: 0.9333 - val_loss: 2.1288 - val_categorical_accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2098 - categorical_accuracy: 0.9333 - val_loss: 1.7962 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1412 - categorical_accuracy: 0.9649 - val_loss: 1.7805 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0677 - categorical_accuracy: 0.9754 - val_loss: 2.1648 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1364 - categorical_accuracy: 0.9719 - val_loss: 2.1926 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.4088 - categorical_accuracy: 0.9193 - val_loss: 2.1584 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1907 - categorical_accuracy: 0.9439 - val_loss: 1.5564 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.4314 - categorical_accuracy: 0.9439 - val_loss: 3.6878 - val_categorical_accuracy: 0.0667 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2573 - categorical_accuracy: 0.9228 - val_loss: 2.2118 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1806 - categorical_accuracy: 0.9474 - val_loss: 2.5657 - val_categorical_accuracy: 0.1333 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0743 - categorical_accuracy: 0.9860 - val_loss: 3.6890 - val_categorical_accuracy: 0.1333 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0712 - categorical_accuracy: 0.9930 - val_loss: 1.9382 - val_categorical_accuracy: 0.1333 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0371 - categorical_accuracy: 0.9965 - val_loss: 1.9143 - val_categorical_accuracy: 0.1333 - lr: 2.0000e-04\n",
      "Epoch 24/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1342 - categorical_accuracy: 0.9860 - val_loss: 1.9891 - val_categorical_accuracy: 0.1333 - lr: 2.0000e-04\n",
      "Epoch 25/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0714 - categorical_accuracy: 0.9719 - val_loss: 2.1104 - val_categorical_accuracy: 0.1333 - lr: 2.0000e-04\n",
      "Epoch 26/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0270 - categorical_accuracy: 0.9965 - val_loss: 2.1889 - val_categorical_accuracy: 0.1333 - lr: 2.0000e-04\n",
      "Epoch 27/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0934 - categorical_accuracy: 0.9789 - val_loss: 2.2197 - val_categorical_accuracy: 0.1333 - lr: 2.0000e-04\n",
      "Epoch 28/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0684 - categorical_accuracy: 0.9860 - val_loss: 2.2429 - val_categorical_accuracy: 0.2000 - lr: 1.0000e-04\n",
      "Epoch 29/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0192 - categorical_accuracy: 0.9930 - val_loss: 2.3086 - val_categorical_accuracy: 0.2000 - lr: 1.0000e-04\n",
      "Epoch 30/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0375 - categorical_accuracy: 0.9789 - val_loss: 2.3670 - val_categorical_accuracy: 0.2000 - lr: 1.0000e-04\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 30, 64)            90368     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 30, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 30, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 258346 (1009.16 KB)\n",
      "Trainable params: 257962 (1007.66 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Model saved at: Model_data/2\\2.keras\n",
      "Model weights saved at: Model_data/2\\2_weights.keras\n",
      "X shape: (300, 30, 288), y shape: (300, 10)\n",
      "Epoch 1/1000\n",
      "9/9 [==============================] - 5s 97ms/step - loss: 2.2935 - categorical_accuracy: 0.1684 - val_loss: 2.2616 - val_categorical_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 2.0341 - categorical_accuracy: 0.2807 - val_loss: 2.1297 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 1.6027 - categorical_accuracy: 0.4421 - val_loss: 2.1073 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 1.1155 - categorical_accuracy: 0.5930 - val_loss: 2.0731 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.9454 - categorical_accuracy: 0.6842 - val_loss: 1.8640 - val_categorical_accuracy: 0.2667 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.5691 - categorical_accuracy: 0.8035 - val_loss: 1.8790 - val_categorical_accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.4844 - categorical_accuracy: 0.8211 - val_loss: 1.7621 - val_categorical_accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2841 - categorical_accuracy: 0.8947 - val_loss: 1.5468 - val_categorical_accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2642 - categorical_accuracy: 0.9088 - val_loss: 2.0673 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2322 - categorical_accuracy: 0.9333 - val_loss: 2.1646 - val_categorical_accuracy: 0.2667 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1517 - categorical_accuracy: 0.9649 - val_loss: 1.9651 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1804 - categorical_accuracy: 0.9509 - val_loss: 2.2262 - val_categorical_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1793 - categorical_accuracy: 0.9614 - val_loss: 4.6006 - val_categorical_accuracy: 0.2667 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1556 - categorical_accuracy: 0.9579 - val_loss: 2.6170 - val_categorical_accuracy: 0.3333 - lr: 2.0000e-04\n",
      "Epoch 15/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0625 - categorical_accuracy: 0.9860 - val_loss: 6.0469 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n",
      "Epoch 16/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0966 - categorical_accuracy: 0.9754 - val_loss: 8.3606 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n",
      "Epoch 17/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0804 - categorical_accuracy: 0.9754 - val_loss: 7.2623 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n",
      "Epoch 18/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0568 - categorical_accuracy: 0.9754 - val_loss: 4.9956 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n",
      "Epoch 19/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0399 - categorical_accuracy: 0.9930 - val_loss: 3.3843 - val_categorical_accuracy: 0.2667 - lr: 1.0000e-04\n",
      "Epoch 20/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0612 - categorical_accuracy: 0.9754 - val_loss: 2.1494 - val_categorical_accuracy: 0.2667 - lr: 1.0000e-04\n",
      "Epoch 21/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2422 - categorical_accuracy: 0.9684 - val_loss: 1.0656 - val_categorical_accuracy: 0.7333 - lr: 1.0000e-04\n",
      "Epoch 22/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0464 - categorical_accuracy: 0.9825 - val_loss: 0.6819 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 23/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0538 - categorical_accuracy: 0.9930 - val_loss: 0.5519 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 24/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0368 - categorical_accuracy: 0.9965 - val_loss: 0.4525 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 25/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0635 - categorical_accuracy: 0.9789 - val_loss: 0.3843 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 26/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0570 - categorical_accuracy: 0.9825 - val_loss: 0.3216 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 27/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0757 - categorical_accuracy: 0.9789 - val_loss: 0.2591 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 28/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0320 - categorical_accuracy: 0.9860 - val_loss: 0.2181 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 29/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0521 - categorical_accuracy: 0.9895 - val_loss: 0.1820 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 30/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0416 - categorical_accuracy: 0.9895 - val_loss: 0.1675 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 31/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0498 - categorical_accuracy: 0.9825 - val_loss: 0.1683 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 32/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0089 - categorical_accuracy: 1.0000 - val_loss: 0.1517 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 33/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0244 - categorical_accuracy: 0.9860 - val_loss: 0.1158 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 34/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0161 - categorical_accuracy: 0.9930 - val_loss: 0.0847 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 35/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0470 - categorical_accuracy: 0.9789 - val_loss: 0.0608 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 36/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0352 - categorical_accuracy: 0.9789 - val_loss: 0.0408 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 37/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0361 - categorical_accuracy: 0.9895 - val_loss: 0.0213 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 38/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0297 - categorical_accuracy: 0.9895 - val_loss: 0.0111 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 39/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0290 - categorical_accuracy: 0.9930 - val_loss: 0.0048 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 40/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0156 - categorical_accuracy: 0.9930 - val_loss: 0.0023 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 41/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0050 - categorical_accuracy: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 42/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0291 - categorical_accuracy: 0.9895 - val_loss: 7.4584e-04 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 43/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0303 - categorical_accuracy: 0.9895 - val_loss: 4.6227e-04 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 44/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0677 - categorical_accuracy: 0.9825 - val_loss: 4.4306e-04 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 45/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0157 - categorical_accuracy: 0.9965 - val_loss: 4.7065e-04 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 46/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0250 - categorical_accuracy: 0.9895 - val_loss: 2.9127e-04 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 47/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0163 - categorical_accuracy: 0.9895 - val_loss: 1.9398e-04 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 48/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0331 - categorical_accuracy: 0.9930 - val_loss: 1.1087e-04 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 49/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0280 - categorical_accuracy: 0.9895 - val_loss: 5.2282e-05 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 50/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0119 - categorical_accuracy: 0.9930 - val_loss: 2.8895e-05 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 51/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0522 - categorical_accuracy: 0.9754 - val_loss: 1.8683e-05 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 52/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0098 - categorical_accuracy: 1.0000 - val_loss: 1.8397e-05 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 53/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0264 - categorical_accuracy: 0.9895 - val_loss: 1.6887e-05 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 54/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0189 - categorical_accuracy: 0.9930 - val_loss: 2.0018e-05 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 55/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0209 - categorical_accuracy: 0.9930 - val_loss: 2.5731e-05 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 56/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0040 - categorical_accuracy: 1.0000 - val_loss: 2.2712e-05 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 57/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0125 - categorical_accuracy: 1.0000 - val_loss: 1.0458e-05 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 58/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0273 - categorical_accuracy: 0.9895 - val_loss: 4.4345e-06 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 59/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0361 - categorical_accuracy: 0.9860 - val_loss: 2.1616e-06 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 60/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0130 - categorical_accuracy: 0.9965 - val_loss: 1.2318e-06 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 61/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0118 - categorical_accuracy: 0.9965 - val_loss: 8.5036e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 62/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0339 - categorical_accuracy: 0.9860 - val_loss: 5.0068e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 63/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0165 - categorical_accuracy: 0.9965 - val_loss: 2.8610e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 64/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.0027 - categorical_accuracy: 1.0000 - val_loss: 2.3842e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 65/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0251 - categorical_accuracy: 0.9965 - val_loss: 2.6226e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 66/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0295 - categorical_accuracy: 0.9895 - val_loss: 5.2452e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 67/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0109 - categorical_accuracy: 0.9965 - val_loss: 5.7220e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 68/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0125 - categorical_accuracy: 0.9930 - val_loss: 5.1657e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 69/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0151 - categorical_accuracy: 0.9930 - val_loss: 2.3842e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 70/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0277 - categorical_accuracy: 0.9930 - val_loss: 7.1526e-08 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 71/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0058 - categorical_accuracy: 1.0000 - val_loss: 1.0331e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 72/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0259 - categorical_accuracy: 0.9860 - val_loss: 1.1126e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 73/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0110 - categorical_accuracy: 0.9965 - val_loss: 2.7815e-07 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 74/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0158 - categorical_accuracy: 0.9965 - val_loss: 4.0372e-06 - val_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 75/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0137 - categorical_accuracy: 1.0000 - val_loss: 0.2122 - val_categorical_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 76/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1484 - categorical_accuracy: 0.9860 - val_loss: 0.3863 - val_categorical_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 77/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0279 - categorical_accuracy: 0.9930 - val_loss: 0.4713 - val_categorical_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 78/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0173 - categorical_accuracy: 0.9965 - val_loss: 0.4731 - val_categorical_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 79/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0165 - categorical_accuracy: 0.9965 - val_loss: 0.4328 - val_categorical_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 80/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0275 - categorical_accuracy: 0.9895 - val_loss: 0.3371 - val_categorical_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 81/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0268 - categorical_accuracy: 0.9965 - val_loss: 0.1074 - val_categorical_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 82/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0163 - categorical_accuracy: 0.9930 - val_loss: 1.1584 - val_categorical_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 83/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0187 - categorical_accuracy: 0.9895 - val_loss: 1.4589 - val_categorical_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 30, 64)            90368     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 30, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 30, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 258346 (1009.16 KB)\n",
      "Trainable params: 257962 (1007.66 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Model saved at: Model_data/3\\3.keras\n",
      "Model weights saved at: Model_data/3\\3_weights.keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tkinter import filedialog, Tk\n",
    "import random\n",
    "\n",
    "# Function to create and save the model\n",
    "def create_and_save_model(DATA_PATH=\"\"):\n",
    "    if DATA_PATH == \"\":\n",
    "        root = Tk()\n",
    "        root.withdraw()  # Hide the root window\n",
    "        DATA_PATH = filedialog.askdirectory(title=\"Select the model data folder\")\n",
    "\n",
    "    model_name = os.path.basename(DATA_PATH)  # Use the folder name as the model name\n",
    "    actions_csv_path = os.path.join(DATA_PATH, 'actions.csv')\n",
    "\n",
    "    if not os.path.exists(actions_csv_path):\n",
    "        print(f\"Error: actions.csv not found in {DATA_PATH}\")\n",
    "        return\n",
    "\n",
    "    # Load actions from the CSV file\n",
    "    actions = []\n",
    "    with open(actions_csv_path, mode='r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            actions.append(row[1])  # Assuming the action is in the second column\n",
    "\n",
    "    # Create a label map\n",
    "    label_map = {label: num for num, label in enumerate(actions)}\n",
    "    sequence_length = 30  # Set your sequence length\n",
    "\n",
    "    sequences, labels = [], []\n",
    "    for action in actions:\n",
    "        action_dir = os.path.join(DATA_PATH, \"actions\", action)\n",
    "        if os.path.exists(action_dir):\n",
    "            sequence_dirs = [d for d in os.listdir(action_dir) if d.startswith('seq_')]\n",
    "\n",
    "            for sequence_dir in sequence_dirs:\n",
    "                sequence_path = os.path.join(action_dir, sequence_dir)\n",
    "                window = []\n",
    "                \n",
    "                # Determine if the sequence should start from a random frame\n",
    "                random_start = random.choice([True, False])\n",
    "                if random_start:\n",
    "                    start_frame = random.randint(0, sequence_length - 1)\n",
    "                else:\n",
    "                    start_frame = 0\n",
    "\n",
    "                # Collect frames, looping back to the beginning if needed\n",
    "                for i in range(sequence_length):\n",
    "                    frame_num = (start_frame + i) % sequence_length\n",
    "                    npy_path = os.path.join(sequence_path, f\"frame_{frame_num}.npy\")\n",
    "                    \n",
    "                    if os.path.exists(npy_path):\n",
    "                        res = np.load(npy_path)\n",
    "                        window.append(res)\n",
    "                    else:\n",
    "                        print(f\"Frame {npy_path} not found, skipping this sequence.\")\n",
    "                        break\n",
    "                \n",
    "                # Only include complete sequences\n",
    "                if len(window) == sequence_length:\n",
    "                    sequences.append(window)\n",
    "                    labels.append(label_map[action])\n",
    "\n",
    "    if len(sequences) == 0 or len(labels) == 0:\n",
    "        print(\"No valid sequences or labels found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(sequences)\n",
    "    y = to_categorical(labels).astype(int)\n",
    "\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "    # Set up TensorBoard logging\n",
    "    log_dir = os.path.join('Logs')\n",
    "    tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    # Define the model with increased variability, Dropout, and BatchNormalization\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, activation='relu', input_shape=(sequence_length, X.shape[2])),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        LSTM(128, return_sequences=True, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        LSTM(64, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(len(actions), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    # Define callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=13, restore_best_weights=True)  # Increased patience by 3\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=1000, validation_data=(X_test, y_test), \n",
    "              callbacks=[reduce_lr, early_stopping, tb_callback])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Create the Models directory if it doesn't exist\n",
    "    model_dir = DATA_PATH\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Save the model and weights in `.keras` format\n",
    "    model_path = os.path.join(model_dir, f'{model_name}.keras')\n",
    "    weights_path = os.path.join(model_dir, f'{model_name}_weights.keras')\n",
    "\n",
    "    model.save(model_path)  # Save the entire model in `.keras` format\n",
    "    model.save_weights(weights_path)  # Save only the model weights in `.keras` format\n",
    "\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "    print(f\"Model weights saved at: {weights_path}\")\n",
    "\n",
    "# Example usage\n",
    "create_and_save_model(\"Model_data/1\")\n",
    "create_and_save_model(\"Model_data/2\")\n",
    "create_and_save_model(\"Model_data/3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, filedialog\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Global variables\n",
    "new_csv_path = \"\"\n",
    "is_random = False\n",
    "num_actions = 0\n",
    "specific_action_lines = []\n",
    "actions = []\n",
    "open_window = None  # Keep track of the currently open window\n",
    "\n",
    "# Load actions from the CSV file\n",
    "def load_actions():\n",
    "    global actions\n",
    "    global CSV_FILE\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        print(f\"CSV_FILE '{CSV_FILE}' does not exist.\")\n",
    "        return\n",
    "    with open(CSV_FILE, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        actions = [row for row in reader]\n",
    "\n",
    "# Close the current window if one is open\n",
    "def close_current_window():\n",
    "    global open_window\n",
    "    if open_window is not None:\n",
    "        open_window.destroy()\n",
    "        open_window = None\n",
    "\n",
    "# First window to create a new model directory\n",
    "def create_new_model():\n",
    "    def on_create_model():\n",
    "        model_name = \"Model_data/\" + model_name_entry.get()\n",
    "        global DATA_PATH\n",
    "        DATA_PATH = model_name\n",
    "\n",
    "        if not model_name_entry.get():\n",
    "            messagebox.showerror(\"Error\", \"Model name cannot be empty!\")\n",
    "            return\n",
    "        model_dir = os.path.join(os.getcwd(), model_name)\n",
    "\n",
    "        if os.path.exists(model_dir):\n",
    "            messagebox.showerror(\"Error\", \"Directory already exists!\")\n",
    "        else:\n",
    "            os.makedirs(model_dir)\n",
    "            messagebox.showinfo(\"Success\", f\"Directory '{model_name}' created!\")\n",
    "            open_create_from_window(model_name, model_window)  # Pass model_window to close later\n",
    "\n",
    "    # Close the current window before opening a new one\n",
    "    close_current_window()\n",
    "\n",
    "    # GUI for model creation\n",
    "    model_window = tk.Toplevel(root)\n",
    "    model_window.title(\"Create New Model\")\n",
    "    model_window.geometry(\"400x200\")\n",
    "    \n",
    "    # Track the newly opened window\n",
    "    global open_window\n",
    "    open_window = model_window\n",
    "\n",
    "    model_name_label = tk.Label(model_window, text=\"Enter new model name:\")\n",
    "    model_name_label.pack(pady=10)\n",
    "\n",
    "    model_name_entry = tk.Entry(model_window)\n",
    "    model_name_entry.pack(pady=10)\n",
    "\n",
    "    create_model_button = tk.Button(model_window, text=\"Create Model\", command=on_create_model)\n",
    "    create_model_button.pack(pady=10)\n",
    "\n",
    "# New window \"Create From...\"\n",
    "def open_create_from_window(model_name, previous_window):\n",
    "    def open_csv_creation():\n",
    "        previous_window.destroy()  # Close previous window\n",
    "        create_action_selection_window(model_name, create_from_window)\n",
    "\n",
    "    def open_existing_videos():\n",
    "        previous_window.destroy()  # Close previous window\n",
    "        from_videos()  # Open the from_videos window\n",
    "\n",
    "    def open_existing_mp_data():\n",
    "        previous_window.destroy()  # Close previous window\n",
    "        from_mp_data()  # Handle the existing MP Data option\n",
    "\n",
    "    # Close the current window before opening a new one\n",
    "    close_current_window()\n",
    "\n",
    "    create_from_window = tk.Toplevel(root)\n",
    "    create_from_window.title(\"Create From...\")\n",
    "    create_from_window.geometry(\"400x300\")\n",
    "\n",
    "    # Track the newly opened window\n",
    "    global open_window\n",
    "    open_window = create_from_window\n",
    "\n",
    "    # Label\n",
    "    create_from_label = tk.Label(create_from_window, text=\"How would you like to create new model?\")\n",
    "    create_from_label.pack(pady=10)\n",
    "\n",
    "    # Fresh Button\n",
    "    fresh_button = tk.Button(create_from_window, text=\"Fresh\", command=open_csv_creation)\n",
    "    fresh_button.pack(pady=10)\n",
    "    tk.Label(create_from_window, text=\"Create model by selecting actions, preprocess and process videos, extract MP data, test, train, and save model.\").pack(pady=5)\n",
    "\n",
    "    # Existing Videos Button\n",
    "    existing_videos_button = tk.Button(create_from_window, text=\"Existing MP Data\", command=open_existing_videos)\n",
    "    existing_videos_button.pack(pady=10)\n",
    "    tk.Label(create_from_window, text=\"Create models from existing MP data, test, train, and save model.\").pack(pady=5)\n",
    "\n",
    "\n",
    "\n",
    "# Function to process videos\n",
    "def from_videos(processed=True):\n",
    "    def begin_processing_and_extraction():\n",
    "        model_action_path = os.path.join(DATA_PATH, \"actions\")\n",
    "        os.makedirs(model_action_path, exist_ok=True)\n",
    "\n",
    "        if not processed:\n",
    "            # Process actions based on CSV_FILE\n",
    "            for line in actions:\n",
    "                action_name_dir = os.path.join(model_action_path, line[1])\n",
    "                os.makedirs(action_name_dir, exist_ok=True)\n",
    "                i = 0\n",
    "                for element in line[2:]:  # Skipping pos 0 and pos 1 elements\n",
    "                    mp_path = os.path.join(action_name_dir, \"seq_\"+str(i))\n",
    "                    full_MP_Extraction(element, mp_path)\n",
    "                    i += 1\n",
    "                data_refine(action_name_dir)\n",
    "                print(\"sdfkjhasdfkljhsdaflkjasdhfkljsadhflkajsdhfasldkjfhasdlkjfhasdjklf\")\n",
    "            messagebox.showinfo(\"Success\", \"Processing and extraction complete!\")\n",
    "        else:\n",
    "            # Open file explorer for user to select directory\n",
    "            selected_path = filedialog.askdirectory()\n",
    "            if selected_path:\n",
    "                actions_dir = os.path.join(selected_path, 'actions')\n",
    "                actions_csv = os.path.join(selected_path, 'actions.csv')\n",
    "                if os.path.exists(actions_dir) and os.path.exists(actions_csv):\n",
    "                    # Copy the 'actions' directory and 'actions.csv' into DATA_PATH\n",
    "                    target_actions_dir = os.path.join(DATA_PATH, 'actions')\n",
    "                    target_actions_csv = os.path.join(DATA_PATH, 'actions.csv')\n",
    "                    os.makedirs(target_actions_dir, exist_ok=True)\n",
    "\n",
    "                    # Copy actions directory and actions.csv\n",
    "                    os.system(f'cp -r \"{actions_dir}\" \"{target_actions_dir}\"')\n",
    "                    os.system(f'cp \"{actions_csv}\" \"{target_actions_csv}\"')\n",
    "                    \n",
    "                    messagebox.showinfo(\"Success\", \"Preprocessed data has been copied!\")\n",
    "                else:\n",
    "                    messagebox.showerror(\"Error\", \"Selected directory must contain 'actions' and 'actions.csv'.\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"No directory selected.\")\n",
    "        create_and_save_model(DATA_PATH)\n",
    "\n",
    "    # Close the current window before opening a new one\n",
    "    close_current_window()\n",
    "\n",
    "    video_window = tk.Toplevel(root)\n",
    "    video_window.title(\"From Videos\")\n",
    "    video_window.geometry(\"400x300\")\n",
    "\n",
    "    # Track the newly opened window\n",
    "    global open_window\n",
    "    open_window = video_window\n",
    "\n",
    "    # Instruction label\n",
    "    if not processed:\n",
    "        instruction_label = tk.Label(video_window, text=\"Processing unprocessed data from CSV.\")\n",
    "    else:\n",
    "        instruction_label = tk.Label(video_window, text=\"Select preprocessed data directory.\")\n",
    "    instruction_label.pack(pady=10)\n",
    "\n",
    "    # Add a button to start processing\n",
    "    if processed:\n",
    "        select_data_button = tk.Button(video_window, text=\"Select Data Directory\", command=begin_processing_and_extraction)\n",
    "        select_data_button.pack(pady=20)\n",
    "    else:\n",
    "        process_button = tk.Button(video_window, text=\"Begin Processing and Extraction\", command=begin_processing_and_extraction)\n",
    "        process_button.pack(pady=20)\n",
    "\n",
    "# Process MP data (placeholder)\n",
    "def from_mp_data():\n",
    "    close_current_window()\n",
    "    mp_window = tk.Toplevel(root)\n",
    "    mp_window.title(\"From MP Data\")\n",
    "    mp_window.geometry(\"400x300\")\n",
    "\n",
    "    global open_window\n",
    "    open_window = mp_window\n",
    "\n",
    "    tk.Label(mp_window, text=\"This is the MP Data window.\").pack()\n",
    "\n",
    "# Second window to select actions for creating CSV\n",
    "def create_action_selection_window(model_name, previous_window):\n",
    "    def on_checkbox_selected(selected_var):\n",
    "        # Deselect other checkboxes when one is selected\n",
    "        if selected_var == all_actions_var:\n",
    "            number_of_actions_var.set(0)\n",
    "            specify_actions_var.set(0)\n",
    "            number_of_actions_frame.pack_forget()\n",
    "            specify_actions_frame.pack_forget()\n",
    "        elif selected_var == number_of_actions_var:\n",
    "            all_actions_var.set(0)\n",
    "            specify_actions_var.set(0)\n",
    "            number_of_actions_frame.pack(pady=10)\n",
    "            specify_actions_frame.pack_forget()\n",
    "        elif selected_var == specify_actions_var:\n",
    "            all_actions_var.set(0)\n",
    "            number_of_actions_var.set(0)\n",
    "            number_of_actions_frame.pack_forget()\n",
    "            specify_actions_frame.pack(pady=10)\n",
    "\n",
    "    def on_create_model_csv():\n",
    "        global is_random\n",
    "        new_csv_path = os.path.join(os.getcwd(), model_name, \"actions.csv\")\n",
    "        with open(new_csv_path, 'w', newline='') as new_csv:\n",
    "            writer = csv.writer(new_csv)\n",
    "            if all_actions_var.get():\n",
    "                # Copy all actions from the CSV file\n",
    "                for row in actions:\n",
    "                    writer.writerow(row)\n",
    "            elif number_of_actions_var.get():\n",
    "                # Select a number of actions\n",
    "                num = int(num_actions_entry.get())\n",
    "                is_random = random_checkbox_var.get()\n",
    "                if num > len(actions):\n",
    "                    messagebox.showerror(\"Error\", f\"Number exceeds total actions ({len(actions)}).\")\n",
    "                    return\n",
    "                selected_actions = random.sample(actions, num) if is_random else actions[:num]\n",
    "                for row in selected_actions:\n",
    "                    writer.writerow(row)\n",
    "            elif specify_actions_var.get():\n",
    "                # Write specific actions to new CSV\n",
    "                if not specific_action_lines:\n",
    "                    messagebox.showerror(\"Error\", \"No actions selected.\")\n",
    "                    return\n",
    "                for line in specific_action_lines:\n",
    "                    writer.writerow(line)\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"No option selected.\")\n",
    "                return\n",
    "\n",
    "        messagebox.showinfo(\"Success\", f\"CSV created at: {new_csv_path}\")\n",
    "        previous_window.destroy()  # Close the CSV selection window\n",
    "        global CSV_FILE\n",
    "        CSV_FILE = new_csv_path\n",
    "        load_actions()\n",
    "        from_videos(False)  # Call `from_videos` after creating CSV\n",
    "\n",
    "    # Close the current window before opening a new one\n",
    "    close_current_window()\n",
    "\n",
    "    action_window = tk.Toplevel(root)\n",
    "    previous_window.destroy()  # Close the \"Create From\" window\n",
    "    action_window.title(\"Select Actions\")\n",
    "    action_window.geometry(\"600x600\")\n",
    "\n",
    "    global open_window\n",
    "    open_window = action_window\n",
    "\n",
    "    # --- Action Selection Checkboxes ---\n",
    "    all_actions_var = tk.IntVar(value=1)\n",
    "    number_of_actions_var = tk.IntVar(value=0)\n",
    "    specify_actions_var = tk.IntVar(value=0)\n",
    "\n",
    "    all_actions_checkbox = tk.Checkbutton(action_window, text=\"All Actions\", variable=all_actions_var, command=lambda: on_checkbox_selected(all_actions_var))\n",
    "    all_actions_checkbox.pack(anchor='w')\n",
    "\n",
    "    number_of_actions_checkbox = tk.Checkbutton(action_window, text=\"# of Actions\", variable=number_of_actions_var, command=lambda: on_checkbox_selected(number_of_actions_var))\n",
    "    number_of_actions_checkbox.pack(anchor='w')\n",
    "\n",
    "    specify_actions_checkbox = tk.Checkbutton(action_window, text=\"Specify Actions\", variable=specify_actions_var, command=lambda: on_checkbox_selected(specify_actions_var))\n",
    "    specify_actions_checkbox.pack(anchor='w')\n",
    "\n",
    "    # --- Number of Actions Section ---\n",
    "    number_of_actions_frame = tk.Frame(action_window)\n",
    "    tk.Label(number_of_actions_frame, text=\"Number of Actions:\").pack(side=tk.LEFT)\n",
    "    num_actions_entry = tk.Entry(number_of_actions_frame)\n",
    "    num_actions_entry.pack(side=tk.LEFT, padx=5)\n",
    "    random_checkbox_var = tk.IntVar()\n",
    "    random_checkbox = tk.Checkbutton(number_of_actions_frame, text=\"Random\", variable=random_checkbox_var)\n",
    "    random_checkbox.pack(side=tk.LEFT)\n",
    "\n",
    "    # --- Specify Actions Section ---\n",
    "    specify_actions_frame = tk.Frame(action_window)\n",
    "\n",
    "    tk.Label(specify_actions_frame, text=\"Search Actions:\").pack()\n",
    "    search_entry_var = tk.StringVar()\n",
    "    search_entry = tk.Entry(specify_actions_frame, textvariable=search_entry_var)\n",
    "    search_entry.pack(pady=5)\n",
    "\n",
    "    search_results_listbox = tk.Listbox(specify_actions_frame, selectmode=tk.MULTIPLE)\n",
    "    search_results_listbox.pack(pady=5)\n",
    "\n",
    "    tk.Label(specify_actions_frame, text=\"Selected Actions:\").pack()\n",
    "    selected_actions_listbox = tk.Listbox(specify_actions_frame)\n",
    "    selected_actions_listbox.pack(pady=5)\n",
    "\n",
    "    # Create model CSV button\n",
    "    create_csv_button = tk.Button(action_window, text=\"Create CSV\", command=on_create_model_csv)\n",
    "    create_csv_button.pack(pady=20)\n",
    "\n",
    "    # Initially display the appropriate frame based on checkbox selection\n",
    "    on_checkbox_selected(all_actions_var)\n",
    "\n",
    "# Function for opening the Settings window\n",
    "def open_settings():\n",
    "    # Close the current window before opening a new one\n",
    "    close_current_window()\n",
    "\n",
    "    # Create a new window for settings\n",
    "    settings_window = tk.Toplevel(root)\n",
    "    settings_window.title(\"Settings\")\n",
    "    settings_window.geometry(\"300x200\")\n",
    "\n",
    "    # Track the newly opened window\n",
    "    global open_window\n",
    "    open_window = settings_window\n",
    "\n",
    "    tk.Label(settings_window, text=\"Settings Window\").pack()\n",
    "\n",
    "# Main window setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Main Window\")\n",
    "root.geometry(\"300x150\")  # Set the size of the main window\n",
    "\n",
    "# Add the \"Run Model\" button (you'll need to implement `interpretation` function if needed)\n",
    "run_model_button = tk.Button(root, text=\"Run Model\", command=lambda: print(\"Run model pressed\"))\n",
    "run_model_button.pack(pady=20)\n",
    "\n",
    "# Add the \"Create Model\" button to open model creation\n",
    "create_model_button = tk.Button(root, text=\"Create Model\", command=create_new_model)\n",
    "create_model_button.pack(pady=20)\n",
    "\n",
    "# Add the \"Settings\" button\n",
    "settings_button = tk.Button(root, text=\"Settings\", command=open_settings)\n",
    "settings_button.pack(pady=10)\n",
    "\n",
    "# Load actions from CSV when the program starts\n",
    "load_actions()\n",
    "\n",
    "# Run the main loop to display the window\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs456",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
